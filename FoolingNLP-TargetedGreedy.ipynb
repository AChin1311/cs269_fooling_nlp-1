{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import data_utils\n",
    "import pickle\n",
    "import attacks\n",
    "import random\n",
    "from keras.models import load_model\n",
    "\n",
    "import greedy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = 'tokenizer.pickle'\n",
    "TEXT_DATA_DIR = '20_newsgroup/'\n",
    "MODEL_PATH = '20news_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer, inverse_tokenizer = data_utils.load_tokenizer(TOKENIZER_PATH)\n",
    "model = load_model(MODEL_PATH)\n",
    "labels =  list(sorted(os.listdir(TEXT_DATA_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_file, true_label = data_utils.pick_random_file(TEXT_DATA_DIR)\n",
    "file_text = data_utils.load_textfile(sample_file)\n",
    "file_features = data_utils.process_text(tokenizer, file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueLabel = alt.atheism\n",
      "Predicted \"alt.atheism\" with 0.195061 .\n"
     ]
    }
   ],
   "source": [
    "pred_scores = model.predict(file_features)\n",
    "orig_prediction = np.argmax(pred_scores[0])\n",
    "print('TrueLabel = %s' %true_label)\n",
    "print('Predicted \"%s\" with %f .' %(labels[orig_prediction], pred_scores[0][orig_prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Attack (targeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre compute the required probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMPUTE_PROBS = True\n",
    "if COMPUTE_PROBS:\n",
    "    num_words = len(inverse_tokenizer)\n",
    "    topics_words, topics_words_probs = greedy_utils.compute_topic_words(TEXT_DATA_DIR, tokenizer, \n",
    "                                                                  labels, num_words, num_cands=20000,ret_count=500)\n",
    "    with open('topic_words.pickle', 'wb') as handle:\n",
    "        pickle.dump(topics_words, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('topc_words_probs.pickle', 'wb') as handle:\n",
    "        pickle.dump(topics_words_probs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('topic_words.pickle', 'rb') as handle:\n",
    "        topics_words = pickle.load(handle)\n",
    "    with open('topc_words_probs.pickle', 'rb') as handle:\n",
    "        topics_words_probs = pickle.load( handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('./glove.6B', 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index.get('dbstu1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for label :  alt.atheism\n",
      "dbstu1 ingles* rosenau* mozumder macalstr mccullou wwc* razor* meng* nm0w \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  comp.graphics\n",
      "bolson* roundoff* gifconverter denali* fli* hsi* renderer* normals* spline* bib* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  comp.os.ms-windows.misc\n",
      "claebaur sqk w4wg mk7 uwt b8g p47 a865 'as' hm9 \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  comp.sys.ibm.pc.hardware\n",
      "1542* viewsonic* p9000 dcoleman dce* cyrix* uart* micron* ebosco sectors* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  comp.sys.mac.hardware\n",
      "c650 q700 ntx iici unplug* fpu* lcii oscillators* 68020* binhex \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  comp.windows.x\n",
      "doit* xtappcontext lxmu xrdb ftms openwinhome olvwm xfilesearchpath elin* dpy \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  misc.forsale\n",
      "armegedon spiderman* obo* nikon* dryer* typewriter* thd* turtles* vouchers* unregistered* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  rec.autos\n",
      "unforgiven* diesels* gibbonsa mustang* lexus* shaz* spiros* shafts* oils* maxima* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  rec.motorcycles\n",
      "tharp* kotb* wibbled kryptonite* jnmoyne cjackson* jeq pillion* harleys* bethd \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  rec.sport.baseball\n",
      "rbi* griffey* vesterman beastmaster* garvey* catchers* overburdened* gehrig* 05pm bunning* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  rec.sport.hockey\n",
      "skrudland* roughing* ufsa foligno* cullen* admirals* lapointe* nylander* gaudreau* etxonss \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  sci.crypt\n",
      "toal* hellman* vesselin* ciphertext* rfcs* tcmay scif* cryptosystem* plaintext* pem* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  sci.electronics\n",
      "n8wed wtm* hams* conductor* wiring* resistor* dialing* mjm pcb* 35894 \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  sci.med\n",
      "phenylalanine* zisfein sinusitis* alzheimer's glutamate* o157* insulin* lesions* conjugate* placebo* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  sci.space\n",
      "bursters* steinly goldin* billboard* krumins* sherzer* satellites* energetic* worden* comet* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  soc.religion.christian\n",
      "revdak creps ahmadiyya* boswell's virgilio* baptize* papal* gifted* protestant* zane* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  talk.politics.guns\n",
      "hci* homicides* 2923 rti* vmcms roby* gunners* firearms* handguns* manes* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  talk.politics.mideast\n",
      "farid* gaya* tarihi mecmuasi pinkas* macedonian* 8563446 muharrerat tarih spied* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  talk.politics.misc\n",
      "queers* miyazawa* c5rusq steveh socialized* tenants* kinsey* teel* panicking* reich* \n",
      "-----\n",
      "\n",
      "Top 10 words for label :  talk.religion.misc\n",
      "chorion* psyrobtw vaux* kendig* isscck iniquity* fait* hypocrite* samaritan* intentionality* \n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words for \n",
    "for ix, label in enumerate(labels):\n",
    "    print(\"Top 10 words for label : \", label)\n",
    "    for j in range(10):\n",
    "        w_idx = topics_words[ix][j]\n",
    "        w = inverse_tokenizer[w_idx]\n",
    "        print(w, end=\"\")\n",
    "        w_vec = embeddings_index.get(w)\n",
    "        has_embedding = not (w_vec is None)\n",
    "        if has_embedding:\n",
    "            print(\"*\", end=\" \")\n",
    "        else:\n",
    "            print(\"\", end=\" \")\n",
    "        #print(w + has_embedding, end=\" \" )\n",
    "    print(\"\\n-----\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greedy_attack = attacks.GreedyAttack(model, topics_words, topics_words_probs, temp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_labels = [x for x in range(len(labels)) if x != orig_prediction]\n",
    "random_target = np.random.choice(other_labels)\n",
    "print('Random target = %s' %labels[random_target])\n",
    "x_orig = file_features.copy()\n",
    "x_adv, o_hist, t_hist = greedy_attack.attack(x_orig, random_target, limit=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_adv is None:\n",
    "    print('Attack failed. !')\n",
    "else:\n",
    "    adv_prediction = np.argmax(model.predict(x_adv))\n",
    "    print('Attack succeeded after %d iterations.' %(len(t_hist)))\n",
    "    print('Original class: %s, Attack class: %s' %(labels[orig_prediction], labels[adv_prediction]))\n",
    "    print(\"Number of changed words = %d (%0.2f %%)\"\n",
    "          %(np.count_nonzero(x_adv != x_orig),\n",
    "            100*(np.count_nonzero(x_adv != x_orig))/np.count_nonzero(x_orig)))\n",
    "    plt.plot(o_hist, 'g', label=labels[orig_prediction])\n",
    "    plt.plot(t_hist, 'r', label=labels[adv_prediction])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_text = data_utils.reconstruct_text(inverse_tokenizer, file_features[0])\n",
    "adv_text = data_utils.reconstruct_text(inverse_tokenizer, x_adv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_html, adv_html = data_utils.render_attack(orig_text, adv_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<b> Original Text </b>\")\n",
    "HTML(orig_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<b> Adversarial Text </b>\")\n",
    "HTML(adv_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_files = [data_utils.pick_random_file(TEXT_DATA_DIR) for _ in range(500)]\n",
    "files_, topics = zip(*random_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greedy_attack = attacks.GreedyAttack(model, topics_words, topics_words_probs, temp=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_cnt = 0\n",
    "success_cnt = 0\n",
    "cnt_all = 0\n",
    "dist_list = []\n",
    "attack_list = []\n",
    "class_cnt = [0 for _ in range(len(labels))]\n",
    "attack_matrix = np.zeros((len(labels), len(labels)))\n",
    "for idx in range(len(files_)):\n",
    "    f_name = files_[idx]\n",
    "    f_label = topics[idx]\n",
    "    x_test = data_utils.load_textfile(f_name)\n",
    "    x_orig = data_utils.process_text(tokenizer, x_test)\n",
    "    orig_pred = np.argmax(model.predict(x_orig))\n",
    "    true_label = [i for i,x in enumerate(labels) if x == f_label][0]\n",
    "    if orig_pred != true_label:\n",
    "        # skip\n",
    "        continue\n",
    "    class_cnt[true_label] += 1\n",
    "    # pertrub to every other label\n",
    "    other_labels = [x for x in range(len(labels)) if x != true_label]\n",
    "    cnt_all += 1\n",
    "    \n",
    "    for t_label in other_labels:\n",
    "        x_adv,_,_ = greedy_attack.attack(x_orig, t_label)\n",
    "        if x_adv is None:\n",
    "            failed_cnt += 1\n",
    "        else:\n",
    "            success_cnt += 1\n",
    "            adv_pred = np.argmax(model.predict(x_adv))\n",
    "            assert(adv_pred == t_label)\n",
    "            attack_matrix[orig_pred, adv_pred] += 1\n",
    "            dist_list.append(np.count_nonzero(x_adv != x_orig) / np.count_nonzero(x_orig))\n",
    "\n",
    "\n",
    "    if idx % 50 == 0:\n",
    "        print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = 100.0 * success_cnt/ (cnt_all*19)\n",
    "print('Success rate = %0.2f %%' %success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CDF\n",
    "num_bins = 50\n",
    "counts, bin_edges = np.histogram(dist_list, bins=num_bins, density=False)\n",
    "cdf = np.cumsum(counts)\n",
    "cdf = cdf / (success_cnt)\n",
    "plt.plot(bin_edges[1:], cdf*100)\n",
    "plt.axhline(y=50,linewidth=1, color='r', linestyle='--')\n",
    "plt.xlabel('%% change')\n",
    "plt.ylabel('CDF of success')\n",
    "plt.savefig('cdf_greedy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_p = attack_matrix /np.array(class_cnt).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.palplot(sns.color_palette(\"RdBu_r\", 7)) Label\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "fig, ax = plt.subplots(figsize=((10,10)))\n",
    "sns.heatmap(attack_p, annot=True, fmt=\"0.1f\",\n",
    "            yticklabels=labels, xticklabels=labels, cbar=False, cmap=\"OrRd\"\n",
    "           \n",
    "           )\n",
    "plt.xlabel('Target Label', fontsize=16)\n",
    "plt.ylabel('Source Label', fontsize=16)\n",
    "ax.xaxis.set_label_position('top')\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('greedy_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Picking similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('./glove.6B', 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_words = topics_words[orig_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.count_nonzero(x_orig==x) for x in orig_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_most_similar(src_word, target_words, inverse_tokenizer, embedding_index):\n",
    "    cnt = 0\n",
    "    shortest_dist = 100000000\n",
    "    src_vector = embedding_index.get(inverse_tokenizer[src_word])\n",
    "    ret = -1\n",
    "    for w_idx in target_words:\n",
    "        w = inverse_tokenizer[w_idx]\n",
    "        embedding_vector = embeddings_index.get(w)\n",
    "        if embedding_vector is not None:\n",
    "            dist = np.sum((src_vector - embedding_vector)**2)\n",
    "            print(inverse_tokenizer[w_idx], ' ' , dist)\n",
    "            if dist < shortest_dist and dist != 0:\n",
    "                shortest_dist = dist\n",
    "                ret = w_idx\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_word=630\n",
    "ret_word = pick_most_similar(src_word, topics_words[9], inverse_tokenizer, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_tokenizer[ret_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_tokenizer[src_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import data_utils\n",
    "import pickle\n",
    "import attacks\n",
    "import random\n",
    "from keras.models import load_model\n",
    "\n",
    "import greedy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = 'tokenizer.pickle'\n",
    "TEXT_DATA_DIR = '20_newsgroup/'\n",
    "MODEL_PATH = '20news_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, inverse_tokenizer = data_utils.load_tokenizer(TOKENIZER_PATH)\n",
    "model = load_model(MODEL_PATH)\n",
    "labels =  list(sorted(os.listdir(TEXT_DATA_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file, true_label = data_utils.pick_random_file(TEXT_DATA_DIR)\n",
    "file_text = data_utils.load_textfile(sample_file)\n",
    "file_features = data_utils.process_text(tokenizer, file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = model.predict(file_features)\n",
    "orig_prediction = np.argmax(pred_scores[0])\n",
    "print('TrueLabel = %s' %true_label)\n",
    "print('Predicted \"%s\" with %f .' %(labels[orig_prediction], pred_scores[0][orig_prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomised Attack (Non-targeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Random attack will pick and replace words randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_PROBS = False\n",
    "if COMPUTE_PROBS:\n",
    "    num_words = len(inverse_tokenizer)\n",
    "    topics_words, topics_words_probs = greedy_utils.compute_topic_words(TEXT_DATA_DIR, tokenizer, \n",
    "                                                                  labels, num_words)\n",
    "    with open('topic_words.pickle', 'wb') as handle:\n",
    "        pickle.dump(topics_words, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('topc_words_probs.pickle', 'wb') as handle:\n",
    "        pickle.dump(topics_words_probs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('topic_words.pickle', 'rb') as handle:\n",
    "        topics_words = pickle.load(handle)\n",
    "    with open('topc_words_probs.pickle', 'rb') as handle:\n",
    "        topics_words_probs = pickle.load( handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy attack choose word from document that has the highest p(original class|word) and replace it with the one most similar with it in the top 200 words with p(goal class|word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_attack = attacks.GreedyAttack(model, topics_words, topics_words_probs, temp=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_labels = [x for x in range(len(labels)) if x != orig_prediction]\n",
    "random_target = np.random.choice(other_labels)\n",
    "print('Random target = %s' %labels[random_target])\n",
    "x_orig = file_features.copy()\n",
    "x_adv, o_hist, t_hist = greedy_attack.attack(x_orig, random_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_adv is None:\n",
    "    print('Attack failed. !')\n",
    "else:\n",
    "    adv_prediction = np.argmax(model.predict(x_adv))\n",
    "    print('Attack succeeded after %d iterations.' %(len(t_hist)))\n",
    "    print('Original class: %s, Attack class: %s' %(labels[orig_prediction], labels[adv_prediction]))\n",
    "    print(\"Number of changed words = %d (%0.2f %%)\"\n",
    "          %(np.count_nonzero(x_adv != x_orig),\n",
    "            100*(np.count_nonzero(x_adv != x_orig))/np.count_nonzero(x_orig)))\n",
    "    plt.plot(o_hist, 'g', label=labels[orig_prediction] + ' - greedy attack v2')\n",
    "    plt.plot(t_hist, 'r', label=labels[adv_prediction] + '- greedy attack v2')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('ohist.p', 'wb') as handle:\n",
    "        pickle.dump(o_hist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('thist.p', 'wb') as handle:\n",
    "        pickle.dump(t_hist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_text = data_utils.reconstruct_text(inverse_tokenizer, file_features[0])\n",
    "adv_text = data_utils.reconstruct_text(inverse_tokenizer, x_adv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_html, adv_html = data_utils.render_attack(orig_text, adv_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<b> Original Text </b>\")\n",
    "HTML(orig_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<b> Adversarial Text </b>\")\n",
    "HTML(adv_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment across all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20_newsgroup/alt.atheism\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "num_docs_per_class = 3   ##should be changed to a larger number (ex. 50) later\n",
    "class_ind = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    print(path)\n",
    "    docnum = num_docs_per_class\n",
    "    if os.path.isdir(path):\n",
    "        to_test = random.sample(sorted(os.listdir(path)), num_docs_per_class)\n",
    "        changed_avg = [0]*20\n",
    "        for fname in to_test:\n",
    "            file_text = data_utils.load_textfile(os.path.join(path, fname))\n",
    "            file_features = data_utils.process_text(tokenizer, file_text)\n",
    "            pred_scores = model.predict(file_features)\n",
    "            orig_prediction = np.argmax(pred_scores[0])\n",
    "            if(orig_prediction!=class_ind):\n",
    "                docnum = docnum - 1\n",
    "                continue\n",
    "            other_labels = [x for x in range(len(labels)) if x != orig_prediction]\n",
    "            for label in other_labels:\n",
    "                #print(label)\n",
    "                greedy_attack = attacks.GreedyAttack(model, topics_words, topics_words_probs, temp=0.15)\n",
    "                x_orig = file_features.copy()\n",
    "                x_adv, o_hist, t_hist = greedy_attack.attack(x_orig, label)\n",
    "                changed_avg[label] = changed_avg[label] + np.count_nonzero(x_adv != x_orig)/np.count_nonzero(x_orig)        \n",
    "        newList = [x/docnum if docnum!=0 else -1 for x in changed_avg]\n",
    "        print(newList)\n",
    "        results = results + newList\n",
    "    class_ind = class_ind + 1    \n",
    "print(results)   \n",
    "with open('results.p', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
